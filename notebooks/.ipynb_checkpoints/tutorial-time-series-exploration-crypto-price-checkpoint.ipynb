{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quick intro**\n",
    "\n",
    "I wanted to see a notebook / tutorial that would take me through the **basics of working with time series**.  \n",
    "\n",
    "Most notebooks I saw were either not very rigorous or they took me straight into price prediction using some methods, which is not what I needed right away.  \n",
    "\n",
    "So I wrote my own introductory notebook. I acquired most of the information I used here through reading <a href='https://machinelearningmastery.com/introduction-to-time-series-forecasting-with-python/'>Introduction to Time Series Forecasting With Python</a> by Jason Brownlee (took a few days to read an implement).   \n",
    "\n",
    "What this notebook is:  \n",
    "- a good starting point for understanding time series data and how it differs from problems with other type of tabular data \n",
    "- a cookbook we can use for exploration when starting to work with a new dataset\n",
    "\n",
    "What this notebook is not:\n",
    "- it is not about prediction. It stops at exploration and understanding the data.\n",
    "- it's not meant for advanced practitioners of asset price prediction - unless you want to revisit some concepts.  \n",
    "\n",
    "### Contents\n",
    "\n",
    "[1. Quick overview](#1.-Quick-overview)  \n",
    "[2. Dataset description](#2.-Dataset-description)  \n",
    "[3. Basic trading data visualization](#3.-Basic-trading-data-visualization)  \n",
    "[4. Preprocessing](#4.-Preprocessing)  \n",
    "[5. Feature engineering](#5.-Feature-engineering)  \n",
    "[6. Typical time series visualizations](#6.-Typical-time-series-visualizations)  \n",
    "[7. Power transforms](#7.-Power-transforms)  \n",
    "[8. Temporal structure](#.8-Temporal-structure-of-time-series-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Quick overview\n",
    "\n",
    "This dataset was made available on Kaggle as part of the <a href='https://www.kaggle.com/c/g-research-crypto-forecasting/overview'>G-Research Crypto forecasting competition</a>. The challenge proposed by G-Research was to predict price returns across a bundle of major cryptocurrencies for which we have approximately 3 years of historical price data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T01:58:49.744731Z",
     "iopub.status.busy": "2022-01-24T01:58:49.743993Z",
     "iopub.status.idle": "2022-01-24T01:58:49.768335Z",
     "shell.execute_reply": "2022-01-24T01:58:49.767662Z",
     "shell.execute_reply.started": "2022-01-24T01:58:49.744649Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T01:58:49.770060Z",
     "iopub.status.busy": "2022-01-24T01:58:49.769850Z",
     "iopub.status.idle": "2022-01-24T01:58:50.504715Z",
     "shell.execute_reply": "2022-01-24T01:58:50.503722Z",
     "shell.execute_reply.started": "2022-01-24T01:58:49.770036Z"
    }
   },
   "outputs": [],
   "source": [
    "data_folder = \"../input/g-research-crypto-forecasting/\"\n",
    "!ls $data_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Available files (provided by the organizers of the competition):\n",
    "- **train.csv**\n",
    "- **asset_details.csv**\n",
    "- example_sample_submission.csv\n",
    "- example_test.csv\n",
    "- supplemental_train.csv\n",
    "\n",
    "Only the first two seem interesting for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T01:58:50.506859Z",
     "iopub.status.busy": "2022-01-24T01:58:50.506326Z",
     "iopub.status.idle": "2022-01-24T01:59:49.357170Z",
     "shell.execute_reply": "2022-01-24T01:59:49.356326Z",
     "shell.execute_reply.started": "2022-01-24T01:58:50.506823Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "crypto_df = pd.read_csv(os.path.join(data_folder, 'train.csv'))\n",
    "\n",
    "end = time.time()\n",
    "print(end - start) # just ouf curiosity, see how long it takes to read such a large csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T01:59:49.359657Z",
     "iopub.status.busy": "2022-01-24T01:59:49.359340Z",
     "iopub.status.idle": "2022-01-24T01:59:49.368057Z",
     "shell.execute_reply": "2022-01-24T01:59:49.367274Z",
     "shell.execute_reply.started": "2022-01-24T01:59:49.359616Z"
    }
   },
   "outputs": [],
   "source": [
    "crypto_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are approximately 24 million observations in our time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T01:59:49.370071Z",
     "iopub.status.busy": "2022-01-24T01:59:49.369579Z",
     "iopub.status.idle": "2022-01-24T01:59:49.396518Z",
     "shell.execute_reply": "2022-01-24T01:59:49.395923Z",
     "shell.execute_reply.started": "2022-01-24T01:59:49.370030Z"
    }
   },
   "outputs": [],
   "source": [
    "crypto_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data features**  \n",
    "\n",
    "- **timestamp** - Unix timestamps (the number of seconds elapsed since 1970-01-01 00:00:00.000 UTC). Timestamps in this dataset are multiple of 60, indicating minute-by-minute data.\n",
    "- **Asset_ID** - uniquely identifies the traded coin\n",
    "- **Count** - number of trades executed within the respective minute\n",
    "- **Open, High, Low, Close** - the usual price details for a given unit of time. \n",
    "- **Volume** - amount of units of this coin traded in the particular minute\n",
    "- **VWAP** - The average price of the asset over the time interval, weighted by volume. VWAP is an aggregated form of trade data.\n",
    "- **Target** - Residual log-returns for the asset over a 15 minute horizon <- we know this from the competition's official description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T01:59:49.399066Z",
     "iopub.status.busy": "2022-01-24T01:59:49.398781Z",
     "iopub.status.idle": "2022-01-24T01:59:49.406603Z",
     "shell.execute_reply": "2022-01-24T01:59:49.405947Z",
     "shell.execute_reply.started": "2022-01-24T01:59:49.399041Z"
    }
   },
   "outputs": [],
   "source": [
    "start = crypto_df.iloc[0].timestamp.astype('datetime64[s]')\n",
    "end = crypto_df.iloc[-1].timestamp.astype('datetime64[s]')\n",
    "\n",
    "print(f'Data from {start} until {end}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have approximately 3 years worth of data. This informs the type of time windows we can look at.  \n",
    "\n",
    "For example, if we zoom out at an yearly resolution, we can only compare 2018, 2019 and 2020 (2021 is incomplete)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assets \n",
    "\n",
    "In the list of transactions, assets are referred to by Asset_ID. Let's look into **asset_details.csv** to see what these 'assets' are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T01:59:49.407713Z",
     "iopub.status.busy": "2022-01-24T01:59:49.407500Z",
     "iopub.status.idle": "2022-01-24T01:59:49.422033Z",
     "shell.execute_reply": "2022-01-24T01:59:49.421115Z",
     "shell.execute_reply.started": "2022-01-24T01:59:49.407688Z"
    }
   },
   "outputs": [],
   "source": [
    "asset_details_df = pd.read_csv(os.path.join(data_folder, 'asset_details.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T01:59:49.423381Z",
     "iopub.status.busy": "2022-01-24T01:59:49.422862Z",
     "iopub.status.idle": "2022-01-24T01:59:49.432750Z",
     "shell.execute_reply": "2022-01-24T01:59:49.431995Z",
     "shell.execute_reply.started": "2022-01-24T01:59:49.423352Z"
    }
   },
   "outputs": [],
   "source": [
    "asset_details_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting selection of 14 crypto coins. I say interesting because, as a crypto investor myself, I wonder what IOTA and Dogecoin are doing in there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Basic trading data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a subset of our timeframe (the last 60 entries for example) and restrict the analysis to one asset (which makes it a univariate analysis). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T01:59:49.433930Z",
     "iopub.status.busy": "2022-01-24T01:59:49.433747Z",
     "iopub.status.idle": "2022-01-24T01:59:49.695242Z",
     "shell.execute_reply": "2022-01-24T01:59:49.694457Z",
     "shell.execute_reply.started": "2022-01-24T01:59:49.433910Z"
    }
   },
   "outputs": [],
   "source": [
    "btc_mini_df = crypto_df[crypto_df.Asset_ID == 1].iloc[-60:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T01:59:49.699433Z",
     "iopub.status.busy": "2022-01-24T01:59:49.699175Z",
     "iopub.status.idle": "2022-01-24T01:59:49.713400Z",
     "shell.execute_reply": "2022-01-24T01:59:49.712903Z",
     "shell.execute_reply.started": "2022-01-24T01:59:49.699402Z"
    }
   },
   "outputs": [],
   "source": [
    "btc_mini_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's transform the timestamp into the index of our data. This will facilitate some operations down the road."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T01:59:49.714506Z",
     "iopub.status.busy": "2022-01-24T01:59:49.714164Z",
     "iopub.status.idle": "2022-01-24T01:59:49.723539Z",
     "shell.execute_reply": "2022-01-24T01:59:49.722819Z",
     "shell.execute_reply.started": "2022-01-24T01:59:49.714479Z"
    }
   },
   "outputs": [],
   "source": [
    "btc_mini_df = btc_mini_df.set_index(\"timestamp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the candlestick function from the graph_objects package of plotly to visualize trading data as the common candlestick plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T01:59:49.724755Z",
     "iopub.status.busy": "2022-01-24T01:59:49.724501Z",
     "iopub.status.idle": "2022-01-24T01:59:49.848676Z",
     "shell.execute_reply": "2022-01-24T01:59:49.847811Z",
     "shell.execute_reply.started": "2022-01-24T01:59:49.724732Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(data=[go.Candlestick(x=btc_mini_df.index, \n",
    "                                     open=btc_mini_df['Open'], \n",
    "                                     high=btc_mini_df['High'], \n",
    "                                     low=btc_mini_df['Low'], \n",
    "                                     close=btc_mini_df['Close'])])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Candlestick plots look nice and fancy, but later we'll go beyond this into exploring trends, cycles and seasonality of the dataset.  \n",
    "\n",
    "For now, it's just a way to get a quick look at our data, so we have a mental model of what we're dealing with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Preprocessing  \n",
    "\n",
    "Detecting features for which we have missing values is easy to do. We just look for null values per column.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T01:59:49.850381Z",
     "iopub.status.busy": "2022-01-24T01:59:49.850093Z",
     "iopub.status.idle": "2022-01-24T01:59:50.162068Z",
     "shell.execute_reply": "2022-01-24T01:59:50.161235Z",
     "shell.execute_reply.started": "2022-01-24T01:59:49.850343Z"
    }
   },
   "outputs": [],
   "source": [
    "btc_df = crypto_df[crypto_df.Asset_ID == 1].set_index('timestamp')\n",
    "\n",
    "btc_df.info(show_counts =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, more clearly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T01:59:50.164020Z",
     "iopub.status.busy": "2022-01-24T01:59:50.163346Z",
     "iopub.status.idle": "2022-01-24T01:59:50.201475Z",
     "shell.execute_reply": "2022-01-24T01:59:50.200884Z",
     "shell.execute_reply.started": "2022-01-24T01:59:50.163977Z"
    }
   },
   "outputs": [],
   "source": [
    "btc_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a lot of data is missing. We can drop those rows, impute the values etc. I'm not going to use Target, so I won't go into this right now. \n",
    "\n",
    "The more covert missing data is when we don't have any information at all for a particular minute. Which means that **whole rows may be missing** from our dataset.  \n",
    "\n",
    "We should have one row per minute per asset. Since we extracted the data for a single asset, we expect consecutive rows to have a difference of 60 seconds between their index values.  \n",
    "\n",
    "First step is to look at the **time lag between consecutive entries** in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T01:59:50.202856Z",
     "iopub.status.busy": "2022-01-24T01:59:50.202528Z",
     "iopub.status.idle": "2022-01-24T01:59:50.224519Z",
     "shell.execute_reply": "2022-01-24T01:59:50.223763Z",
     "shell.execute_reply.started": "2022-01-24T01:59:50.202812Z"
    }
   },
   "outputs": [],
   "source": [
    "# we start with 1 instead of 0 since there's nothing to compare the first entry to\n",
    "(btc_df.index[1:]-btc_df.index[:-1]).value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have 78 instances where two consecutive entries are 120 seconds apart, instead of 60 sec, and so on. \n",
    "\n",
    "Because the gaps in data are so small, we can use a simple imputation method: fill in the missing data with the value from the most recent available minute.  \n",
    "\n",
    "This is what the method = 'pad' parameter of the reindex function below does.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T01:59:50.226006Z",
     "iopub.status.busy": "2022-01-24T01:59:50.225814Z",
     "iopub.status.idle": "2022-01-24T01:59:50.523619Z",
     "shell.execute_reply": "2022-01-24T01:59:50.522768Z",
     "shell.execute_reply.started": "2022-01-24T01:59:50.225982Z"
    }
   },
   "outputs": [],
   "source": [
    "btc_df = btc_df.reindex(range(btc_df.index[0],btc_df.index[-1]+60,60), method='pad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T01:59:50.525669Z",
     "iopub.status.busy": "2022-01-24T01:59:50.524857Z",
     "iopub.status.idle": "2022-01-24T01:59:50.558141Z",
     "shell.execute_reply": "2022-01-24T01:59:50.557450Z",
     "shell.execute_reply.started": "2022-01-24T01:59:50.525628Z"
    }
   },
   "outputs": [],
   "source": [
    "(btc_df.index[1:]-btc_df.index[:-1]).value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, no more time gaps in our dataset !  \n",
    "\n",
    "We don't need the index as a timestamp anymore. For future analysis it will be easier to have it as a date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T01:59:50.559479Z",
     "iopub.status.busy": "2022-01-24T01:59:50.559226Z",
     "iopub.status.idle": "2022-01-24T02:00:12.896319Z",
     "shell.execute_reply": "2022-01-24T02:00:12.895827Z",
     "shell.execute_reply.started": "2022-01-24T01:59:50.559413Z"
    }
   },
   "outputs": [],
   "source": [
    "btc_df['datetime'] = btc_df.apply(lambda r: np.float64(r.name).astype('datetime64[s]'), axis=1)\n",
    "\n",
    "btc_df.set_index('datetime', inplace=True);\n",
    "\n",
    "btc_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the usual taxonomy of Machine Learning, we have supervised, unsupervised and reinforcement learning problems. I'm looking to apply supervised learning methods for this data.  \n",
    "\n",
    "The data does not look like the typical supervised learning dataset. For each row, we expect to see several features and a label / value that we're trying to predict based on those features. \n",
    "\n",
    "Let's have a look again at our dataset below. Definitely not what we need, yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:00:12.897418Z",
     "iopub.status.busy": "2022-01-24T02:00:12.897166Z",
     "iopub.status.idle": "2022-01-24T02:00:12.909926Z",
     "shell.execute_reply": "2022-01-24T02:00:12.909144Z",
     "shell.execute_reply.started": "2022-01-24T02:00:12.897395Z"
    }
   },
   "outputs": [],
   "source": [
    "btc_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rephrasing the dataset into a supervised learning dataset. \n",
    "\n",
    "In supervised learning datasets we have entries with a set of features (our **x**) and a label / value for our output variable which we want to predict (our **y**).  \n",
    "\n",
    "In the case of time series, our output **y** is the price at time t. But what are the inputs for a possible prediction model ?   \n",
    "\n",
    "To phrase it differently, for each entry in our dataset we need to have a set of feaures and a label (used for training and then outputted as a prediction of our model).  \n",
    "\n",
    "This means we need to convert our original data:\n",
    "\n",
    "| time | value |\n",
    "| --- | --- |\n",
    "| 1514764860 | 13850.176 | \n",
    "| 1514764920 | 13828.102 |\n",
    "| 1514764980 | 13801.314 |\n",
    "| ... | ... |\n",
    "\n",
    "into something of this form:\n",
    "\n",
    "| x | y |\n",
    "| --- | --- |\n",
    "| x1 | y1 |\n",
    "| x2 | y2 |\n",
    "| x3 | y3 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of features (x) we can create from time series data:  \n",
    "- **date time** features \n",
    "- **lag** features  \n",
    "- **window** features   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Date time features\n",
    "\n",
    "Again, our data looks like this:  \n",
    "\n",
    "| timestamp | value |\n",
    "| --- | --- |\n",
    "| 1514764860 | 13850.176 | \n",
    "| 1514764920 | 13828.102 |\n",
    "| 1514764980 | 13801.314 |\n",
    "| ... | ... |\n",
    "\n",
    "We can transform the timestamp into day, hour, min, this way creating three new features.  \n",
    "\n",
    "What types of questions we can answer with this approach ? We can imagine a problem where we're trying to predict the price for a specific time on a specific day.  \n",
    "\n",
    "This makes more sense if our data was temperatures throughout the day, where we know there are daily cycles, yearly cycles etc, rather than stocks, but we're just using this for illustration purposes for now.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:00:12.911534Z",
     "iopub.status.busy": "2022-01-24T02:00:12.911244Z",
     "iopub.status.idle": "2022-01-24T02:00:12.921193Z",
     "shell.execute_reply": "2022-01-24T02:00:12.920694Z",
     "shell.execute_reply.started": "2022-01-24T02:00:12.911493Z"
    }
   },
   "outputs": [],
   "source": [
    "btc_mini_df = btc_df[-7201:].copy(deep=True)  # the 7201 comes from me cheating and looking ahead into the results, trying to get full days of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:00:12.922573Z",
     "iopub.status.busy": "2022-01-24T02:00:12.922179Z",
     "iopub.status.idle": "2022-01-24T02:00:12.939700Z",
     "shell.execute_reply": "2022-01-24T02:00:12.938941Z",
     "shell.execute_reply.started": "2022-01-24T02:00:12.922545Z"
    }
   },
   "outputs": [],
   "source": [
    "btc_mini_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:00:12.940980Z",
     "iopub.status.busy": "2022-01-24T02:00:12.940676Z",
     "iopub.status.idle": "2022-01-24T02:00:17.878240Z",
     "shell.execute_reply": "2022-01-24T02:00:17.877473Z",
     "shell.execute_reply.started": "2022-01-24T02:00:12.940940Z"
    }
   },
   "outputs": [],
   "source": [
    "btc_mini_df['time'] = btc_mini_df.apply(lambda r:r.name, axis=1) # I need to move timestamp back into a column\n",
    "\n",
    "# and parse it into year, month, day and hour\n",
    "btc_mini_df['year'] = [btc_mini_df.iloc[i].time.year for i in range(len(btc_mini_df))]  \n",
    "btc_mini_df['month'] = [btc_mini_df.iloc[i].time.month for i in range(len(btc_mini_df))]\n",
    "btc_mini_df['day'] = [btc_mini_df.iloc[i].time.day for i in range(len(btc_mini_df))]\n",
    "btc_mini_df['hour'] = [btc_mini_df.iloc[i].time.hour for i in range(len(btc_mini_df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:00:17.879652Z",
     "iopub.status.busy": "2022-01-24T02:00:17.879375Z",
     "iopub.status.idle": "2022-01-24T02:00:17.897647Z",
     "shell.execute_reply": "2022-01-24T02:00:17.896747Z",
     "shell.execute_reply.started": "2022-01-24T02:00:17.879623Z"
    }
   },
   "outputs": [],
   "source": [
    "btc_mini_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:00:17.898981Z",
     "iopub.status.busy": "2022-01-24T02:00:17.898752Z",
     "iopub.status.idle": "2022-01-24T02:00:17.916181Z",
     "shell.execute_reply": "2022-01-24T02:00:17.915615Z",
     "shell.execute_reply.started": "2022-01-24T02:00:17.898944Z"
    }
   },
   "outputs": [],
   "source": [
    "# average data per hour \n",
    "tmp = btc_mini_df.groupby(['year', 'month', 'day', 'hour']).mean()\n",
    "\n",
    "# restore the multilevel index created by groupby into the year, month, day, hour columns that we created earlier\n",
    "tmp.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:00:17.917654Z",
     "iopub.status.busy": "2022-01-24T02:00:17.917211Z",
     "iopub.status.idle": "2022-01-24T02:00:17.929169Z",
     "shell.execute_reply": "2022-01-24T02:00:17.928367Z",
     "shell.execute_reply.started": "2022-01-24T02:00:17.917622Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = ['year', 'month', 'day', 'hour', 'Close']\n",
    "\n",
    "tmp[cols].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that would be our training data for a supervised learning model.\n",
    "\n",
    "Trying to predict coin prices based on day and hour will likely be a poor model.  \n",
    "\n",
    "But we can enrich these features with additional ones. Here are a few candidates:\n",
    "- Weekend or not.\n",
    "- Season of the year.\n",
    "- Business quarter of the year.  \n",
    "etc  \n",
    "\n",
    "I'm not convinced this would help much, so I'll explore further ways to build features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Lag features  \n",
    "\n",
    "This is the typical way in which time series data is tranformed into a supervised learning problem.  \n",
    "\n",
    "The idea is to predict the value at the current timestamp based on the value from the previous timestamp(s).   \n",
    "\n",
    "**Nomenclature:**  \n",
    "- because we are predicting a single variable, this is called **univariate**  \n",
    "- the number of previous time steps we use in our prediction is called the **width of the time window** or **the lag**\n",
    "- if we predict only one future time step we are doing **one-step forecast**. We can also perform **multi-step forecast** and predict multiple next steps at once \n",
    "- this method is also called **sliding window**, with a window width of 1 in our case below.\n",
    "\n",
    "Initial dataset:  \n",
    "\n",
    "time | value  \n",
    "\\--------------   \n",
    "t1&emsp;$\\;$|  v1  \n",
    "t2&emsp;$\\;$|  v2  \n",
    "t3&emsp;$\\;$|  v3\n",
    "\n",
    "becomes:    \n",
    "\n",
    "x&emsp;| y  \n",
    "\\---------     \n",
    "?$\\;\\;$|  v1      \n",
    "v1$\\;$|  v2  \n",
    "v2$\\;$|  v3  \n",
    "\n",
    "The actual value for time is gone from our data. We don't care about it. We only care about the actual value at the previous point in time.\n",
    "\n",
    "We use pandas' dataframe.shift() function, which shifts values vertically or horizontally, fills in with NaN values and leaves the index as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:00:17.930981Z",
     "iopub.status.busy": "2022-01-24T02:00:17.930545Z",
     "iopub.status.idle": "2022-01-24T02:00:17.944180Z",
     "shell.execute_reply": "2022-01-24T02:00:17.943580Z",
     "shell.execute_reply.started": "2022-01-24T02:00:17.930947Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp = btc_mini_df['Close'] # extract only the Close price\n",
    "\n",
    "lag_df = pd.concat([tmp.shift(1, axis = 0), tmp], axis=1) # downward shift by 1 step \n",
    "\n",
    "# the original price series becomes the time t value, \n",
    "# while the downward shifted series is time t+1\n",
    "lag_df.columns = ['Close(t)', 'Close(t+1)'] \n",
    "\n",
    "lag_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same transformation as above, but this time we include the 3 previous values, for each future time t+1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:00:17.945846Z",
     "iopub.status.busy": "2022-01-24T02:00:17.945196Z",
     "iopub.status.idle": "2022-01-24T02:00:17.962784Z",
     "shell.execute_reply": "2022-01-24T02:00:17.962268Z",
     "shell.execute_reply.started": "2022-01-24T02:00:17.945814Z"
    }
   },
   "outputs": [],
   "source": [
    "lag_df = pd.concat([tmp.shift(3), tmp.shift(2), tmp.shift(1), tmp], axis=1)\n",
    "\n",
    "lag_df.columns = ['Close(t-2)', 'Close(t-1)', 'Close(t)', 'Close(t+1)'] # rename columns for easier read\n",
    "\n",
    "lag_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the lag does not have to be linear: we don't necessarily have to use the previous k values. We can include in the lag window values from the same day last week or the same hour the day before etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Window features  \n",
    "\n",
    "We saw how we can add a window of a certain width as features to be used by our model to forecast the value at future time t+1.  \n",
    "\n",
    "Besides using the raw values from the time window (i.e. Close(t-1), Close(t), etc), we can also compute *summary statistics* of these values and use them as features for prediction.  \n",
    "\n",
    "The most common aggregate value is the mean of the lag window (also called **moving average** or **rolling mean**).\n",
    "\n",
    "time | value  \n",
    "\\--------------   \n",
    "t1&emsp;$\\;$|  v1  \n",
    "t2&emsp;$\\;$|  v2  \n",
    "t3&emsp;$\\;$|  v3  \n",
    "t4&emsp;$\\;$|  v4\n",
    "\n",
    "**shifted**  \n",
    "\n",
    "time | value  \n",
    "\\--------------   \n",
    "t1&emsp;$\\;$|  ?  \n",
    "t2&emsp;$\\;$|  v1  \n",
    "t3&emsp;$\\;$|  v2  \n",
    "t4&emsp;$\\;$|  v3\n",
    "\n",
    "shifted, applied **rolling mean** (aka moving average ) with a window size of 2  \n",
    "\n",
    "time | value  \n",
    "\\--------------   \n",
    "t1&emsp;$\\;$|  ?  \n",
    "t2&emsp;$\\;$|  ?  \n",
    "t3&emsp;$\\;$|  (v1+v2) / 2  \n",
    "t4&emsp;$\\;$|  (v2+v3) / 2\n",
    "\n",
    "Final dataset\n",
    "\n",
    "| time | mean | Close(t+1) |  \n",
    "| --- | --- | --- |   \n",
    "t1&emsp;$\\;$|  ?  |  v1  |  \n",
    "t2&emsp;$\\;$|  ?  |  v2  |  \n",
    "t3&emsp;$\\;$|  (v1+v2) / 2  |  v3  |  \n",
    "t4&emsp;$\\;$|  (v2+v3) / 2  |  v4  |  \n",
    "\n",
    "The moving average can be used as a naive prediction model: predict for next day the average of the last *w* days (where *w* is the width of the moving average window). But first we are supposed to have *stationary* data (havign no obvious upward or downward trend and no seasonality), so we'll need some further preprocessing down the road. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:00:17.968555Z",
     "iopub.status.busy": "2022-01-24T02:00:17.967832Z",
     "iopub.status.idle": "2022-01-24T02:00:17.984076Z",
     "shell.execute_reply": "2022-01-24T02:00:17.983256Z",
     "shell.execute_reply.started": "2022-01-24T02:00:17.968513Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp = btc_mini_df['Close'] # extract only the Close price\n",
    "\n",
    "lag_df = tmp.shift(1) # downward shift by 1\n",
    "\n",
    "window = lag_df.rolling(window=2) # rolling window size of 2\n",
    "means = window.mean() # compute the means for the rolling windows\n",
    "\n",
    "new_df = pd.concat([means, tmp], axis=1) # concatenate the two series vertically\n",
    "\n",
    "new_df.columns = ['mean(t-1,t)', 't+1'] # rename columns for easier reading\n",
    "\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are of course many summary statistics we can use besides the average for the previous time window.    \n",
    "\n",
    "Also, the window does not have to have a fixed width. It can also be a forever expanding window.  \n",
    "\n",
    "| time | value |\n",
    "| --- | --- | \n",
    "| t1 | 1 | \n",
    "| t2 | 2 | \n",
    "| t3 | 3 | \n",
    "| t4 | 4 | \n",
    "| t4 | 5 |\n",
    "\n",
    "**expanding** rolling window  \n",
    "\n",
    "| # |values |\n",
    "| --- | --- |\n",
    "| 0 | 1 2 |\n",
    "| 1 | 1 2 3 |\n",
    "| 2 | 1 2 3 4 |\n",
    "| 4 | 1 2 3 4 5 |\n",
    "\n",
    "final engineered dataset having summay statistics (min, mean and max window values) computed for the **expanding** rolling window\n",
    "\n",
    "| # | min | mean | max | t+1 |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 0 | 1 | 1 | 1 | 2 |\n",
    "| 1 | 1 | 1.5 | 2 | 3 |\n",
    "| 2 | 1 | 2 | 3 | 4 |\n",
    "| 3 | 1 | 2.5 | 4 | 5 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:00:17.986035Z",
     "iopub.status.busy": "2022-01-24T02:00:17.985360Z",
     "iopub.status.idle": "2022-01-24T02:00:17.999396Z",
     "shell.execute_reply": "2022-01-24T02:00:17.998334Z",
     "shell.execute_reply.started": "2022-01-24T02:00:17.985995Z"
    }
   },
   "outputs": [],
   "source": [
    "window = tmp.expanding()\n",
    "\n",
    "dataframe = pd.concat([window.min(), window.mean(), window.max(), tmp.shift(-1)], axis=1)\n",
    "\n",
    "dataframe.columns = ['min', 'mean', 'max', 't+1']\n",
    "\n",
    "print(dataframe.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:00:18.000824Z",
     "iopub.status.busy": "2022-01-24T02:00:18.000624Z",
     "iopub.status.idle": "2022-01-24T02:00:18.007837Z",
     "shell.execute_reply": "2022-01-24T02:00:18.007102Z",
     "shell.execute_reply.started": "2022-01-24T02:00:18.000800Z"
    }
   },
   "outputs": [],
   "source": [
    "# have a look at the initial dataset again, to verify that tha dataframe above is correct\n",
    "tmp.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='log_returns'></a>\n",
    "#### Log returns feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typical feature for assets price time series is the ratio between the current price and the price at the previous time point. This is usually computed as a log of the ratio in time series modelling, because that facilitates some apperations (additions, etc). In this case, they are called **log returns**.\n",
    "\n",
    "To compute the log return, we can simply take the logarithm of the ratio between two consecutive prices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:00:18.009359Z",
     "iopub.status.busy": "2022-01-24T02:00:18.008997Z",
     "iopub.status.idle": "2022-01-24T02:00:18.019210Z",
     "shell.execute_reply": "2022-01-24T02:00:18.018418Z",
     "shell.execute_reply.started": "2022-01-24T02:00:18.009319Z"
    }
   },
   "outputs": [],
   "source": [
    "# helper function to compute the log returns\n",
    "def log_returns(series, periods = 1):\n",
    "    return np.log(series).diff(periods = periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:00:18.020684Z",
     "iopub.status.busy": "2022-01-24T02:00:18.020186Z",
     "iopub.status.idle": "2022-01-24T02:00:18.263984Z",
     "shell.execute_reply": "2022-01-24T02:00:18.263037Z",
     "shell.execute_reply.started": "2022-01-24T02:00:18.020643Z"
    }
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize = (15,4))\n",
    "\n",
    "lret_btc = log_returns(btc_mini_df.Close,1)[1:]\n",
    "\n",
    "plt.plot(lret_btc)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sum up, the three methods above are a few basic and common ways (by the book) in which time series are rephrased into a dataset on which we can apply supervised machine learning methods.  \n",
    "\n",
    "That's it for feature engineering for now. I'll move on to visualizing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Typical time series visualizations  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 types of information to explore in a time series through visualization:\n",
    "\n",
    "<strong>temporal structure:</strong>  \n",
    "&emsp;- line plots  \n",
    "&emsp;- lag plots  \n",
    "&emsp;- autocorrelation plots  \n",
    "<strong>the distribution of observations:</strong>  \n",
    "&emsp;- histograms  \n",
    "&emsp;- density plots  \n",
    "<strong>the change in distribution over time intervals:</strong>  \n",
    "&emsp;- box and whisker plots  \n",
    "&emsp;- heatmap plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Line plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:00:18.265417Z",
     "iopub.status.busy": "2022-01-24T02:00:18.265206Z",
     "iopub.status.idle": "2022-01-24T02:00:18.279668Z",
     "shell.execute_reply": "2022-01-24T02:00:18.278726Z",
     "shell.execute_reply.started": "2022-01-24T02:00:18.265375Z"
    }
   },
   "outputs": [],
   "source": [
    "btc_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:00:18.281162Z",
     "iopub.status.busy": "2022-01-24T02:00:18.280931Z",
     "iopub.status.idle": "2022-01-24T02:00:28.945400Z",
     "shell.execute_reply": "2022-01-24T02:00:28.944588Z",
     "shell.execute_reply.started": "2022-01-24T02:00:18.281136Z"
    }
   },
   "outputs": [],
   "source": [
    "#btc_df.plot(x='datetime', y='Close', figsize=(8,5))\n",
    "btc_df.Close.plot(figsize=(20,5))\n",
    "plt.title('Evolution of BTC price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot is a bit dense, since it contains all the data we had (almost 3 years worth of data, on a 1-minuted resolution).  \n",
    "\n",
    "But it's apparent that we don't see a noticeable pattern (no apperent pattern that repeats itself year after year, for example).  \n",
    "\n",
    "For time series, it can be better to look at and compare plots from the same interval, like day-to-day, year-to-year etc.\n",
    "\n",
    "Let's assume we're naive about crypto prices and we want to investigate whether there is an daily seasonality, so we want to plot it in a more informative way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:00:28.946811Z",
     "iopub.status.busy": "2022-01-24T02:00:28.946575Z",
     "iopub.status.idle": "2022-01-24T02:00:28.956364Z",
     "shell.execute_reply": "2022-01-24T02:00:28.955367Z",
     "shell.execute_reply.started": "2022-01-24T02:00:28.946782Z"
    }
   },
   "outputs": [],
   "source": [
    "print(btc_mini_df.iloc[0].time)\n",
    "print(btc_mini_df.iloc[-2].time)\n",
    "print(btc_mini_df.iloc[-1].time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:00:28.959890Z",
     "iopub.status.busy": "2022-01-24T02:00:28.959668Z",
     "iopub.status.idle": "2022-01-24T02:00:29.648966Z",
     "shell.execute_reply": "2022-01-24T02:00:29.648091Z",
     "shell.execute_reply.started": "2022-01-24T02:00:28.959862Z"
    }
   },
   "outputs": [],
   "source": [
    "groups = btc_mini_df.groupby('day')\n",
    "\n",
    "days = pd.DataFrame()\n",
    "\n",
    "for name, group in groups:\n",
    "    if name == 21: # skip the last day, which seems to be incomplete \n",
    "        continue\n",
    "    days[name] = group.Close.values\n",
    "\n",
    "days.plot(subplots=True, legend=False, figsize=(10,8), title='BTC price evolution throughout the day\\n2021-09-16 to 2021-09-20');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have some domain knowledge, we can say that we're not surprised we don't see any apparent correlation between consecutive days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Histograms and density plots  \n",
    "\n",
    "Some linear time series forecasting methods assume a well-behaved distribution of observations (like a normal distribution). Before doing statistical tests to formally assess the assumption of normality, we can easily visualize our data as a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:00:29.650239Z",
     "iopub.status.busy": "2022-01-24T02:00:29.649991Z",
     "iopub.status.idle": "2022-01-24T02:00:29.899058Z",
     "shell.execute_reply": "2022-01-24T02:00:29.898324Z",
     "shell.execute_reply.started": "2022-01-24T02:00:29.650203Z"
    }
   },
   "outputs": [],
   "source": [
    "btc_df.Close.hist(figsize=(8,5))\n",
    "plt.title('Histogram of closing prices for BTC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the same information from the line plot and no further insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:00:29.900842Z",
     "iopub.status.busy": "2022-01-24T02:00:29.900590Z",
     "iopub.status.idle": "2022-01-24T02:01:03.874965Z",
     "shell.execute_reply": "2022-01-24T02:01:03.874198Z",
     "shell.execute_reply.started": "2022-01-24T02:00:29.900805Z"
    }
   },
   "outputs": [],
   "source": [
    "btc_df.Close.plot(kind='kde')\n",
    "plt.title('Density plot of closing prices for BTC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Box and whisker plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We looked at the distribution of values across the whole timeframe (3 years worth of data).  \n",
    "\n",
    "But it may be useful to examine the distribution in smaller time windows.  \n",
    "\n",
    "Let's group our data by year and visualize it as box and whisker plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:03.876527Z",
     "iopub.status.busy": "2022-01-24T02:01:03.876229Z",
     "iopub.status.idle": "2022-01-24T02:01:03.883611Z",
     "shell.execute_reply": "2022-01-24T02:01:03.882834Z",
     "shell.execute_reply.started": "2022-01-24T02:01:03.876485Z"
    }
   },
   "outputs": [],
   "source": [
    "print(btc_df.iloc[0].name)\n",
    "print(btc_df.iloc[-2].name)\n",
    "print(btc_df.iloc[-1].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:03.885485Z",
     "iopub.status.busy": "2022-01-24T02:01:03.884830Z",
     "iopub.status.idle": "2022-01-24T02:01:03.932342Z",
     "shell.execute_reply": "2022-01-24T02:01:03.931625Z",
     "shell.execute_reply.started": "2022-01-24T02:01:03.885420Z"
    }
   },
   "outputs": [],
   "source": [
    "btc_df.groupby(pd.Grouper(freq='A')).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have the same number of observations for each year. 2020 has one extra day, for example. 2018 is missing a single data point. And 2021 is quite incomplete. \n",
    "Just to proceed fast through this step, I'll use a quick dirty trick and join the data for each year while filling in missing values with NaN. It's not the best thing to do, but for the scope of this analysis it's good enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:03.933646Z",
     "iopub.status.busy": "2022-01-24T02:01:03.933452Z",
     "iopub.status.idle": "2022-01-24T02:01:04.231131Z",
     "shell.execute_reply": "2022-01-24T02:01:04.230410Z",
     "shell.execute_reply.started": "2022-01-24T02:01:03.933621Z"
    }
   },
   "outputs": [],
   "source": [
    "groups = btc_df.groupby(pd.Grouper(freq='A')) # group by year\n",
    "\n",
    "years = pd.DataFrame([])\n",
    "\n",
    "for name, group in groups: # iterate through the years\n",
    "    tmp = group.groupby(pd.Grouper(freq='D')).Close.mean() # compute the daily mean\n",
    "    tmp.index = tmp.index.strftime('%m-%d') # transform the index into 'mm-dd' only\n",
    "    \n",
    "    years = years.join(tmp, rsuffix=name.year, how = \"outer\") # join together yearly series (on the 'mm-dd' index) \n",
    "    \n",
    "years.boxplot(figsize=(8,6))\n",
    "\n",
    "plt.title('Box and whiskers plots for BTC close prices\\n years 2018 to 2020');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Short recap]   \n",
    "A box plot is interpretted like this:\n",
    "\n",
    "- The middle 50% of the data is contained in the block itself. The upper edge (hinge) of the box indicates the 75th percentile of the data set, and the lower hinge indicates the 25th percentile. \n",
    "- The horizontal line inside the box indicates the median value of the data.\n",
    "- If the median line within the box is not equidistant from the hinges, then the data is skewed.\n",
    "- The small horizontal ends of the vertical lines (the \"whiskers\") indicate the minimum and maximum data values, unless outliers are present in which case the whiskers extend to a maximum of 1.5 times the inter-quartile range (the hight of the box).\n",
    "- The points outside the ends of the whiskers are (suspected) outliers.  \n",
    "\n",
    "Insights from the plot above:  \n",
    "- median value for BTC changes slightly in the first 3 years\n",
    "- 2018 and 2020 are quite rich in outliers compared to the other 2\n",
    "- in 2021 BTC price spiked "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Heat map plots  \n",
    "\n",
    "Another way to look at this data is to plot it as a 2D plot and color-encode price values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:04.232435Z",
     "iopub.status.busy": "2022-01-24T02:01:04.232239Z",
     "iopub.status.idle": "2022-01-24T02:01:04.551261Z",
     "shell.execute_reply": "2022-01-24T02:01:04.550480Z",
     "shell.execute_reply.started": "2022-01-24T02:01:04.232411Z"
    }
   },
   "outputs": [],
   "source": [
    "yrs = ['Close', 'Close2019', 'Close2020']\n",
    "\n",
    "plt.matshow(years[yrs].dropna().T, interpolation=None, aspect='auto')\n",
    "\n",
    "plt.title('BTC closing daily price\\nyears 2018->2020')\n",
    "plt.ylabel('years')\n",
    "plt.xlabel('days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we see in the plot above is that matshow seems to scale the colors relative to the whole 3 years worth of data.  \n",
    "We've seen in the box plots above that that 2021 has a much higher mean value than all previous years.  \n",
    "\n",
    "So this upward going trend from one year to the next obscures the evolution of price throughout the year and makes it less visible.  \n",
    "\n",
    "Therefore I'll first normalize (from min 0 to max 1) the closing price for each year and replot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:04.553141Z",
     "iopub.status.busy": "2022-01-24T02:01:04.552521Z",
     "iopub.status.idle": "2022-01-24T02:01:04.893232Z",
     "shell.execute_reply": "2022-01-24T02:01:04.892475Z",
     "shell.execute_reply.started": "2022-01-24T02:01:04.553104Z"
    }
   },
   "outputs": [],
   "source": [
    "norm_years=(years-years.min())/(years.max()-years.min())\n",
    "\n",
    "plt.matshow(norm_years[yrs].dropna().T, interpolation=None, aspect='auto')\n",
    "\n",
    "plt.title('BTC closing daily price\\nyears 2018->2020\\normalized per year')\n",
    "plt.ylabel('years')\n",
    "plt.xlabel('days')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's easier to spot maximum and minimum per year, but we still don't see an pattern across years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about a daily pattern ?  \n",
    "\n",
    "Let's see a heatmap of price during the day for 10 consecutive days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:04.894676Z",
     "iopub.status.busy": "2022-01-24T02:01:04.894340Z",
     "iopub.status.idle": "2022-01-24T02:01:05.344406Z",
     "shell.execute_reply": "2022-01-24T02:01:05.343695Z",
     "shell.execute_reply.started": "2022-01-24T02:01:04.894628Z"
    }
   },
   "outputs": [],
   "source": [
    "groups = btc_df.groupby(pd.Grouper(freq='D'))\n",
    "\n",
    "days = pd.DataFrame([])\n",
    "\n",
    "for i, (name, group) in enumerate(groups):\n",
    "    tmp = group.Close\n",
    "    tmp.index = tmp.index.strftime('%H:%M')\n",
    "    \n",
    "    days = days.join(tmp, rsuffix=name.year, how = \"outer\")\n",
    "    \n",
    "    if i == 9:\n",
    "        print('break')\n",
    "        break\n",
    "    \n",
    "plt.matshow(days.T, interpolation=None, aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still not very informative. This doesn't seem to be the right way to look at assets prices and now we know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Lag plots  \n",
    "\n",
    "Time series data implies a relationship between the value at a time t+1 and values and previous points in time.  \n",
    "\n",
    "The step size we take to go back in time is called **lag** (lag of 1, lag 2 etc).  \n",
    "\n",
    "Pandas provides the lag plot method. Let's examine the plot first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:05.345799Z",
     "iopub.status.busy": "2022-01-24T02:01:05.345571Z",
     "iopub.status.idle": "2022-01-24T02:01:05.553993Z",
     "shell.execute_reply": "2022-01-24T02:01:05.553267Z",
     "shell.execute_reply.started": "2022-01-24T02:01:05.345773Z"
    }
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import lag_plot\n",
    "\n",
    "lag_plot(btc_df.groupby(pd.Grouper(freq='D')).Close.mean())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a strong positive relation between Close price at t and Close price at t+1.  \n",
    "\n",
    "Let's experiment with the lag value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:05.555977Z",
     "iopub.status.busy": "2022-01-24T02:01:05.555255Z",
     "iopub.status.idle": "2022-01-24T02:01:06.404394Z",
     "shell.execute_reply": "2022-01-24T02:01:06.403708Z",
     "shell.execute_reply.started": "2022-01-24T02:01:05.555931Z"
    }
   },
   "outputs": [],
   "source": [
    "daily_df = btc_df.groupby(pd.Grouper(freq='D')).Close.mean()\n",
    "daily_values = pd.DataFrame(daily_df.values)\n",
    "\n",
    "lags = 8\n",
    "columns = [daily_values]\n",
    "\n",
    "for i in range(1,(lags + 1)):\n",
    "    columns.append(daily_values.shift(i)) # downward shift by i positions\n",
    "\n",
    "dataframe = pd.concat(columns, axis=1)\n",
    "\n",
    "col_names = ['t'] + ['t-'+str(l) for l in range(1,(lags + 1))]\n",
    "\n",
    "dataframe.columns = col_names\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "\n",
    "for i in range(1,(lags + 1)):\n",
    "    ax = plt.subplot(240 + i)\n",
    "    ax.set_title('t vs t-' + str(i))\n",
    "    plt.scatter(x=dataframe['t'].values, y=dataframe['t-'+str(i)].values)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: the price at t correlates quite strongly with the price at previous time points (from t-1 to t-8), for lower price values (approximately half of maximum price). Beyond this value, the correlation becomes weaker the more we go back in time.  \n",
    "\n",
    "We know from the first line plot we looked at that Bitcoin price surged in 2021. Also, common knowledge of crypto assets market says that prices get very volatile when there is a lot of hype and more people get on the trading markets. So, when prices are high, we expect a lot of volatility (more abrupt and erratic price changes across days). This is what the plots above tell us too.  \n",
    "\n",
    "So, what we conclude: tehre's a strong positive relationship with prices on previous days, but not so much when the market becomes hyped and volatile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Autocorrelation plots\n",
    "\n",
    "Lag plots showed a relationship between current price and price at previous time points.  \n",
    "\n",
    "We can also quantify the relationship between the price and the actual lag value.  \n",
    "\n",
    "For a lag=1, we can compute the correlation between current time step value and previous time step value. If we have n time steps in our data, we'll have n-1 correlation values. These values can be anywhere inthe interval [-1,1].  \n",
    "\n",
    "-1 (strongest negative correlation)  \n",
    "0 (no relationshop at all)  \n",
    "1 (strongest positive correlation)  \n",
    "\n",
    "This computation can be done for lag=1 to any lag value we want.\n",
    "\n",
    "Pandas provides the autocorrelation_plot() function for this.  \n",
    "\n",
    "Values outside the dotted lines are statistically significant.  \n",
    "\n",
    "For many types of time series data this will look like a sinusoid, with a decreasing amplitude. It's not the case for asset prices (at least not for those with large liquidity  - traded by many people).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:06.405499Z",
     "iopub.status.busy": "2022-01-24T02:01:06.405299Z",
     "iopub.status.idle": "2022-01-24T02:01:06.649733Z",
     "shell.execute_reply": "2022-01-24T02:01:06.649021Z",
     "shell.execute_reply.started": "2022-01-24T02:01:06.405475Z"
    }
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "btc_days_df = btc_df.groupby(pd.Grouper(freq='D')).Close.mean()\n",
    "\n",
    "autocorrelation_plot(btc_days_df)\n",
    "\n",
    "plt.title('Autocorrelation plot\\ndaily resolution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a statistically significant correlation with up tp 200-ish previous days average prices. The more we go back in time, the lower the correlation, until it starts to become slightly negative (statistically significant).  \n",
    "\n",
    "We know markets have a global evolution (bull / bear market) and local trends (short term excitement for the price of an asset and then slight corrections).\n",
    "\n",
    "So, depending on the time resolution our data has, I expect the autocorrelation plot to capture the underlying long or short term trend. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Power transforms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the linear plot again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:06.651030Z",
     "iopub.status.busy": "2022-01-24T02:01:06.650843Z",
     "iopub.status.idle": "2022-01-24T02:01:40.975315Z",
     "shell.execute_reply": "2022-01-24T02:01:40.974459Z",
     "shell.execute_reply.started": "2022-01-24T02:01:06.651006Z"
    }
   },
   "outputs": [],
   "source": [
    "#btc_df.plot(x='datetime', y='Close', figsize=(8,5))\n",
    "\n",
    "month_sz = 60 * 24 * 30\n",
    "year_sz = 12 * month_sz\n",
    "\n",
    "btc_df.Close.plot(figsize=(20,5))\n",
    "\n",
    "# tail-rolling average transform\n",
    "rolling_m = btc_df.Close.rolling(window=month_sz)\n",
    "rolling_m_mean = rolling_m.mean()\n",
    "rolling_m_mean.plot(color='green')\n",
    "\n",
    "rolling_y = btc_df.Close.rolling(window=year_sz)\n",
    "rolling_y_mean = rolling_y.mean()\n",
    "rolling_y_mean.plot(color='red')\n",
    "\n",
    "plt.legend(['BTC price/min', '30-day rolling avg', '1-year rolling avg'])\n",
    "plt.title('Evolution of BTC price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='non_stationary'></a>\n",
    "We see that the data is non-stationary: there is an **increasing trend**, yet non-linear (the 1-year rolling average in the plot above, going up at least in the last two years) and a **seasonality** component (local ups and downs).  \n",
    "\n",
    "Furthermore, data **variance** (difference between peaks and throughs in the local oscillaions) also changes (higher in the last year). \n",
    "\n",
    "These make it more difficult to model the data with classical statistical methods.\n",
    "\n",
    "A common wasy to remove these and to improve the signal to noise ratio are power transforms.  \n",
    "\n",
    "Two of the commonly used power transforms are: **square root** and **log transform**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Square root\n",
    "\n",
    "Square root helps bring the data into a linear trend and well-behaved distribution (i.e. Gaussian, uniform) *when* the data is quadratic.  \n",
    "\n",
    "Here's an example on made up data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:40.976737Z",
     "iopub.status.busy": "2022-01-24T02:01:40.976515Z",
     "iopub.status.idle": "2022-01-24T02:01:41.555654Z",
     "shell.execute_reply": "2022-01-24T02:01:41.554674Z",
     "shell.execute_reply.started": "2022-01-24T02:01:40.976710Z"
    }
   },
   "outputs": [],
   "source": [
    "quad_data = [i**2 for i in range(1,200)]\n",
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "## quadratic data plots\n",
    "# line plot\n",
    "plt.subplot(221)\n",
    "plt.plot(quad_data)\n",
    "plt.title('Made up data (qudratic)')\n",
    "\n",
    "# histogram\n",
    "plt.subplot(223)\n",
    "plt.hist(quad_data)\n",
    "\n",
    "## square root data plots\n",
    "# linear plots\n",
    "sq_data = np.sqrt(quad_data)\n",
    "plt.subplot(222)\n",
    "plt.plot(sq_data)\n",
    "plt.title('Data after transformation (squared root)')\n",
    "\n",
    "# histogram\n",
    "plt.subplot(224)\n",
    "plt.hist(sq_data)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the result on our real world data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:41.557078Z",
     "iopub.status.busy": "2022-01-24T02:01:41.556829Z",
     "iopub.status.idle": "2022-01-24T02:01:42.803379Z",
     "shell.execute_reply": "2022-01-24T02:01:42.802526Z",
     "shell.execute_reply.started": "2022-01-24T02:01:41.557048Z"
    }
   },
   "outputs": [],
   "source": [
    "data = btc_df.Close\n",
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "## quadratic data plots\n",
    "# line plot\n",
    "plt.subplot(221)\n",
    "plt.plot(data)\n",
    "plt.title('Real data (BTC closing price)')\n",
    "\n",
    "# histogram\n",
    "plt.subplot(223)\n",
    "plt.hist(data)\n",
    "\n",
    "## square root data plots\n",
    "# linear plots\n",
    "sq_data = np.sqrt(data)\n",
    "plt.subplot(222)\n",
    "plt.plot(sq_data)\n",
    "plt.title('Data after transformation (squared root)')\n",
    "\n",
    "# histogram\n",
    "plt.subplot(224)\n",
    "plt.hist(sq_data)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The squared root doesn't seem to help because our data is not quadratic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log transform\n",
    "\n",
    "Log transforms help when the data has an exponential trend, which is often likened to a hokey stick in asset price popular terminology.  \n",
    "\n",
    "We could say we see something like a hockey stick starting in Jan 2021. Let's explore this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:42.804989Z",
     "iopub.status.busy": "2022-01-24T02:01:42.804717Z",
     "iopub.status.idle": "2022-01-24T02:01:44.065976Z",
     "shell.execute_reply": "2022-01-24T02:01:44.065410Z",
     "shell.execute_reply.started": "2022-01-24T02:01:42.804949Z"
    }
   },
   "outputs": [],
   "source": [
    "data = btc_df.Close\n",
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "## initial data plots\n",
    "# line plot\n",
    "plt.subplot(221)\n",
    "plt.plot(data)\n",
    "plt.title('Real data (BTC closing price)')\n",
    "\n",
    "# histogram\n",
    "plt.subplot(223)\n",
    "plt.hist(data)\n",
    "\n",
    "## square root data plots\n",
    "# linear plots\n",
    "sq_data = np.log(data)\n",
    "plt.subplot(222)\n",
    "plt.plot(sq_data)\n",
    "plt.title('Data after transformation (log transform)')\n",
    "\n",
    "# histogram\n",
    "plt.subplot(224)\n",
    "plt.hist(sq_data)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't look like the right power transformation for our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scipy package offers a method to automatically search for the most suitable power transformation for a dataset: <a href='https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html'>boxcox function</a>.\n",
    "\n",
    "[Short overview]  \n",
    "BoxCox procedure identifies the appropriate exponent (lambda = l) to use to transform data into a well-behaved distribution.    \n",
    "\n",
    "The returned lambda value indicates the power to which all data should be raised.  \n",
    "\n",
    "In order to do this, the Box-Cox power transformation searches from lambda = -5 to lambda = +5 until the best value is found.  \n",
    "\n",
    "Best lambda means: the one for which the transformed data's standard deviation is the smallest. In this situation, the data has the highest likelihood – but not a guarantee – to be normally distributed.\n",
    "\n",
    "Table 1: Common Box-Cox Transformations\n",
    "\n",
    "| lambda | transformed Y |   \n",
    "| --- | --- |   \n",
    "| -2 | \tY^(-2) = 1/Y^2\n",
    "| -1 | \tY^(-1) = 1/Y^1\n",
    "| -0.5 | \tY^(-0.5) = 1/(Sqrt(Y))\n",
    "| | \tlog(Y)\n",
    "| 0.5 | \tY^(0.5) = Sqrt(Y)\n",
    "| 1 | \tY^1 = Y\n",
    "| 2 | \tY^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:44.067746Z",
     "iopub.status.busy": "2022-01-24T02:01:44.066884Z",
     "iopub.status.idle": "2022-01-24T02:01:46.512547Z",
     "shell.execute_reply": "2022-01-24T02:01:46.511734Z",
     "shell.execute_reply.started": "2022-01-24T02:01:44.067694Z"
    }
   },
   "outputs": [],
   "source": [
    "# automatically box-cox transform a time series\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "## initial data plots\n",
    "# line plot\n",
    "plt.subplot(221)\n",
    "plt.plot(data)\n",
    "plt.title('Real data (BTC closing price)')\n",
    "\n",
    "# histogram\n",
    "plt.subplot(223)\n",
    "plt.hist(data)\n",
    "\n",
    "## transformed data plots\n",
    "temp_df = pd.DataFrame(btc_df.Close)\n",
    "temp_df.columns = ['Close']\n",
    "\n",
    "temp_df['Close'], lambda_val = boxcox(temp_df['Close'])\n",
    "print(f'best lambda: {lambda_val}')\n",
    "\n",
    "# line plot\n",
    "plt.subplot(222)\n",
    "plt.plot(temp_df['Close'])\n",
    "\n",
    "# histogram\n",
    "plt.subplot(224)\n",
    "plt.hist(temp_df['Close'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best power transformation for our data seems to be the reciprocal square root (lambda = -0.5), according to the boxcox method.  \n",
    "\n",
    "The distribution we obtain is the closest to a normal distribution, so far. But the trend is not linear. We see what happens with data from previous bull markets (2018 highs become more pronounced after this transformation). Too bad we don't have data from December 2017, when the actual peak of the previous bull market happened for BTC prices. My guess is it would approach the level of July 2021 in a transformed plot.  \n",
    "\n",
    "I won't explore this further for the moment, as I'm not convinced this is the right transformation we need for our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Temporal structure of time series data\n",
    "\n",
    "## 8.1 White noise\n",
    "\n",
    "In a [previous section](#log_returns) I computed the log returns for the BTC close price. This plot reminded me of white noise.  \n",
    "\n",
    "We can formally investigate if it actually is white noise .  \n",
    "\n",
    "A time series is white noise if the variables are independent and identically distributed with a mean of zero.  \n",
    "\n",
    "White noise is important in time series forecasting for two reasons:  \n",
    "1. **Prediction**: If a time series is white noise, then it's by definition random and cannot be predicted.\n",
    "2. **Diagnosis**: The errors of a time series model should be white noise. What does this mean ? That the erros contain no information, as all the information from the time series was harnessed by the model itself. And the opposite ? If the erors are not white noise, the model can be improved further.\n",
    "\n",
    "However, it's generally expected that any real like time series will contains a certain amount of white noise.\n",
    "\n",
    "A series is *not* white noise if:\n",
    "- the mean is non zero\n",
    "- the variance changes over time\n",
    "- the is a significant autocorrelation (with lagged values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:46.514044Z",
     "iopub.status.busy": "2022-01-24T02:01:46.513830Z",
     "iopub.status.idle": "2022-01-24T02:01:46.534750Z",
     "shell.execute_reply": "2022-01-24T02:01:46.534067Z",
     "shell.execute_reply.started": "2022-01-24T02:01:46.514001Z"
    }
   },
   "outputs": [],
   "source": [
    "lret_btc = log_returns(btc_df.Close,1)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:46.536058Z",
     "iopub.status.busy": "2022-01-24T02:01:46.535859Z",
     "iopub.status.idle": "2022-01-24T02:01:50.333032Z",
     "shell.execute_reply": "2022-01-24T02:01:50.332063Z",
     "shell.execute_reply.started": "2022-01-24T02:01:46.536033Z"
    }
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize = (15,4))\n",
    "\n",
    "plt.plot(lret_btc)\n",
    "plt.plot(lret_btc.index, [lret_btc.mean()] * len(lret_btc), color='red')\n",
    "plt.legend(['BTC log returns', 'mean'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:50.334829Z",
     "iopub.status.busy": "2022-01-24T02:01:50.334585Z",
     "iopub.status.idle": "2022-01-24T02:01:50.409329Z",
     "shell.execute_reply": "2022-01-24T02:01:50.408482Z",
     "shell.execute_reply.started": "2022-01-24T02:01:50.334788Z"
    }
   },
   "outputs": [],
   "source": [
    "print(lret_btc.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean and std are almost 0. Since variance = std^2, it's also close to 0. \n",
    "\n",
    "Since we have a lot of data, we can split it into shorter time intervals and see if the summary stats change. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:50.411310Z",
     "iopub.status.busy": "2022-01-24T02:01:50.410979Z",
     "iopub.status.idle": "2022-01-24T02:01:50.745313Z",
     "shell.execute_reply": "2022-01-24T02:01:50.744493Z",
     "shell.execute_reply.started": "2022-01-24T02:01:50.411267Z"
    }
   },
   "outputs": [],
   "source": [
    "groups = lret_btc.groupby(pd.Grouper(freq='A')) # group by year\n",
    "\n",
    "years = pd.DataFrame([])\n",
    "\n",
    "for name, group in groups: # iterate through the years\n",
    "    tmp = group.groupby(pd.Grouper(freq='D')).mean() # compute the daily mean\n",
    "    tmp.index = tmp.index.strftime('%m-%d') # transform the index into 'mm-dd' only\n",
    "    \n",
    "    years = years.join(tmp, rsuffix=name.year, how = \"outer\") # join together yearly series (on the 'mm-dd' index) \n",
    "    \n",
    "years.boxplot(figsize=(8,6))\n",
    "plt.ylim([-0.0002, 0.0002])\n",
    "plt.title('Box and whiskers plots for log returns of BTC close prices\\n years 2018 to 2021');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the autocorrelation for the log returns now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:50.747074Z",
     "iopub.status.busy": "2022-01-24T02:01:50.746596Z",
     "iopub.status.idle": "2022-01-24T02:01:51.294130Z",
     "shell.execute_reply": "2022-01-24T02:01:51.293355Z",
     "shell.execute_reply.started": "2022-01-24T02:01:50.747032Z"
    }
   },
   "outputs": [],
   "source": [
    "lret_btc = log_returns(btc_df,1)[1:]\n",
    "\n",
    "temp_df = lret_btc.groupby(pd.Grouper(freq='D')).Close.mean()\n",
    "\n",
    "autocorrelation_plot(temp_df)\n",
    "\n",
    "plt.title('Autocorrelation plot\\nlog returns\\ndaily resolution')\n",
    "plt.ylim([-0.25, 0.25])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spikes past the 95% (solid grey line) and 99% (dotted grey line) confidence levels look like a statistical fluke, in this context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Random walk  \n",
    "\n",
    "A time series is constructed through a *random walk* process as follows:  \n",
    "y(t) = X(t-1) + rnd_step,  \n",
    "where rnd_step is randomly selected from {-1, 1} and os is x(0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:51.295759Z",
     "iopub.status.busy": "2022-01-24T02:01:51.295554Z",
     "iopub.status.idle": "2022-01-24T02:01:51.636319Z",
     "shell.execute_reply": "2022-01-24T02:01:51.635428Z",
     "shell.execute_reply.started": "2022-01-24T02:01:51.295733Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "seed(101)\n",
    "\n",
    "values = [-1 if random() < 0.5 else 1] # x(0)\n",
    "\n",
    "for i in range(1, 1000):\n",
    "    rnd_step = -1 if random() < 0.5 else 1\n",
    "    y_t = values[i-1] + rnd_step\n",
    "    values.append(y_t)\n",
    "\n",
    "plt.figure(figsize=(8,7))\n",
    "\n",
    "# linear plot\n",
    "plt.subplot(211)    \n",
    "plt.plot(values)\n",
    "plt.title('Line plot of a generated random walk time series')\n",
    "\n",
    "# correlogram\n",
    "plt.subplot(212)    \n",
    "autocorrelation_plot(values)\n",
    "plt.title('Autocorrelation plot for a generated random walk time series')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In section [section 6.6](#6.6-Autocorrelation-plots) I plotted the autocorrelation for BTC close price. Let's see it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:51.637902Z",
     "iopub.status.busy": "2022-01-24T02:01:51.637666Z",
     "iopub.status.idle": "2022-01-24T02:01:51.892201Z",
     "shell.execute_reply": "2022-01-24T02:01:51.891416Z",
     "shell.execute_reply.started": "2022-01-24T02:01:51.637868Z"
    }
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "btc_days_df = btc_df.groupby(pd.Grouper(freq='D')).Close.mean()\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "autocorrelation_plot(btc_days_df)\n",
    "\n",
    "plt.title('Autocorrelation plot\\ndaily resolution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autocorrelation plot of the closing price looks exactly like that of a random walk.  \n",
    "\n",
    "In [section 7](#non_stationary) I mentioned our closing price time series looking non-stationary. It's time to apply formal methods to test this assumption.  \n",
    "\n",
    "The *statsmodel* library provides the *adfuller()* method which implements the *Augmented Dickey-Fuller test*.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:51.894164Z",
     "iopub.status.busy": "2022-01-24T02:01:51.893851Z",
     "iopub.status.idle": "2022-01-24T02:01:52.178850Z",
     "shell.execute_reply": "2022-01-24T02:01:52.176207Z",
     "shell.execute_reply.started": "2022-01-24T02:01:51.894123Z"
    }
   },
   "outputs": [],
   "source": [
    "# calculate the stationarity of our closing price data\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# statistical test\n",
    "result = adfuller(btc_days_df)\n",
    "\n",
    "print(f'ADF result: {result[0]} p={result[1]:.3f}') \n",
    "\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print(f'\\t{key}: {value:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null hypothesis (H0) of the ADF test: time series is non-stationary.  \n",
    "Based on the results, H0 cannot be rejected and the chances of this being a fluke are small.  \n",
    "\n",
    "Then the next interesting question is how we can make it stationary.  \n",
    "\n",
    "An easy method is to subtract the previous value for each time step t. That would be an obvious thing to do since we know our dataset looks like a random walk and we know that we obtained a random walk by using a linear function of Close(t-1) to predict Close(t)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:52.180667Z",
     "iopub.status.busy": "2022-01-24T02:01:52.180364Z",
     "iopub.status.idle": "2022-01-24T02:01:52.621030Z",
     "shell.execute_reply": "2022-01-24T02:01:52.620254Z",
     "shell.execute_reply.started": "2022-01-24T02:01:52.180627Z"
    }
   },
   "outputs": [],
   "source": [
    "diff_df = btc_days_df.diff()[1:]\n",
    "\n",
    "plt.figure(figsize=(8,7))\n",
    "\n",
    "# linear plot\n",
    "plt.subplot(211)    \n",
    "plt.plot(diff_df)\n",
    "plt.title('Line plot of the diff time series')\n",
    "\n",
    "# correlogram\n",
    "plt.subplot(212)    \n",
    "autocorrelation_plot(diff_df)\n",
    "plt.title('Autocorrelation plot for the diff time series')\n",
    "plt.ylim([-0.25, 0.25])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sadly, it looks like we get white noise when we remove non-stationarity, which we know contains no structure that we can model and use for prediction.  \n",
    "\n",
    "A naive model for white noise is:  \n",
    "*Close(t) = Close(t-1)*  \n",
    "This is also called the **persistence model**  \n",
    "Just because we know the next value is a function of the current value but we have no way to build a model of this relationship.  \n",
    "\n",
    "The persistence model will be the baseline for any future model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:52.622442Z",
     "iopub.status.busy": "2022-01-24T02:01:52.622193Z",
     "iopub.status.idle": "2022-01-24T02:01:52.770429Z",
     "shell.execute_reply": "2022-01-24T02:01:52.769876Z",
     "shell.execute_reply.started": "2022-01-24T02:01:52.622411Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "# prepare dataset\n",
    "train_size = int(len(btc_days_df) * 0.5)\n",
    "train, test = btc_days_df[0:train_size], btc_days_df[train_size:]\n",
    "\n",
    "# persistence\n",
    "preds = []\n",
    "prev = train[-1]\n",
    "\n",
    "for i in range(len(test)):\n",
    "    preds.append(prev)\n",
    "    prev = test[i]\n",
    "    \n",
    "rmse = sqrt(mean_squared_error(test, preds))\n",
    "print(f'Persistence model RMSE: {rmse:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, I found out that this data is likely a random walk time series. I think that discovering this through analysis is more valuable than having read this commonly known fact directly from some article dealing with forecasting security prices over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Time series decomposition  \n",
    "\n",
    "A time series is conceptualized as having these types of components:\n",
    "1. systematic components\n",
    "    - level = overall average value \n",
    "    - trend = temporary upward or downward movement \n",
    "    - seasonality = a short-term cycle that repeats itself   \n",
    "2. non-systematic components\n",
    "    - random noise\n",
    "    \n",
    "The 4 components are thought to combine in two opssible ways into a time series:  \n",
    "- additive  \n",
    "    *Close(t) = level + trend + seasonality + noise*  \n",
    "        \n",
    "- multiplicative  \n",
    "    *Close(t) = level * trend * seasonality * noise*  \n",
    "    \n",
    "In reality, time series data can have:\n",
    "- both additive and multiplicative components  \n",
    "- both upward and downward trends (especially security price data)  \n",
    "- non-repeating and repeating cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-24T02:01:52.772052Z",
     "iopub.status.busy": "2022-01-24T02:01:52.771323Z",
     "iopub.status.idle": "2022-01-24T02:01:54.573096Z",
     "shell.execute_reply": "2022-01-24T02:01:54.572283Z",
     "shell.execute_reply.started": "2022-01-24T02:01:52.772002Z"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "##whole data\n",
    "data = btc_days_df\n",
    "decomp = seasonal_decompose(data, model='multiplicative')\n",
    "\n",
    "plt.figure(figsize=(20,12))\n",
    "\n",
    "plt.subplot(421)\n",
    "data.plot()\n",
    "plt.ylabel('initial data')\n",
    "plt.title('Whole dataset (3.5 years)')\n",
    "\n",
    "plt.subplot(423)\n",
    "decomp.trend.plot()\n",
    "plt.ylabel('trend')\n",
    "\n",
    "plt.subplot(425)\n",
    "decomp.seasonal.plot()\n",
    "plt.ylabel('seasonal')\n",
    "\n",
    "plt.subplot(427)\n",
    "decomp.resid.plot()\n",
    "plt.ylabel('resdual')\n",
    "\n",
    "##small window\n",
    "data = btc_days_df[:160]\n",
    "decomp = seasonal_decompose(data, model='multiplicative')\n",
    "\n",
    "plt.subplot(422)\n",
    "data.plot()\n",
    "plt.ylabel('initial data')\n",
    "plt.title('Smaller time window (0.5 years)')\n",
    "\n",
    "plt.subplot(424)\n",
    "decomp.trend.plot()\n",
    "plt.ylabel('trend')\n",
    "\n",
    "plt.subplot(426)\n",
    "decomp.seasonal.plot()\n",
    "plt.ylabel('seasonal')\n",
    "\n",
    "plt.subplot(428)\n",
    "decomp.resid.plot()\n",
    "plt.ylabel('resdual')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've reached the temporary end. This is work in progress. There is no conclusion yet. I continue to update this notebook as I read more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
